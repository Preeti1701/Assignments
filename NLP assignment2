
Q1. What are Corpora?

Ans:

A corpus is a collection of authentic text or audio organized into datasets. Authentic here means text written or audio spoken by a native of the language or dialect. A corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

How it is done-->NLTK already defines a list of data paths or directories in nltk.data.path. Our custom corpora must be present within any of these given paths so it can be found by NLTK. We can also create a custom nltk_data directory in our home directory and verify that it is in the list of known paths specified by nltk.data.path.

Q2. What are Tokens?

Ans:

Tokens are the building blocks of Natural Language. Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization

Q3. What are Unigrams, Bigrams, Trigrams?

Ans:

A 1-gram (or unigram) is a one-word sequence. For the above sentence, the unigrams would simply be: “I”, “love”, “reading”, “blogs”, “about”, “data”, “science”, “on”, “Medium”, “Channel”.

A 2-gram (or bigram) is a two-word sequence of words, like “I love”, “love reading”, or “Medium Channel”.

And a 3-gram (or trigram) is a three-word sequence of words like “I love reading”, “about data science” .

Q4. How to generate n-grams from text?

Ans:

The Extract N-Gram Features from Text module creates two types of output: Results dataset: A summary of the analyzed text together with the n-grams that were extracted. Columns that you did not select in the Text column option are passed through to the output

Q5. Explain Lemmatization

Ans:

Lemmatization is a text normalization technique used in Natural Language Processing (NLP). It has been studied for a very long time and lemmatization algorithms have been made since the 1960s. Essentially, lemmatization is a technique that switches any kind of a word to its base root mode. It is a linguistic term that means grouping together words with the same root or lemma but with different inflections or derivatives of meaning so they can be analyzed as one item. The aim is to take away inflectional suffixes and prefixes to bring out the word's dictionary form.

Q6. Explain Stemming

Ans:

In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. ... A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer.

Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).

Q7. Explain Part-of-speech (POS) tagging

Ans:

Part-of-speech (POS) tagging is a popular Natural Language Processing process which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context.

Q8. Explain Chunking or shallow parsing

Ans:

Shallow parsing (also chunking or light parsing) is an analysis of a sentence which first identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and then links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.).

Q9. Explain Noun Phrase (NP) chunking

Ans:

NP is the user-defined name of the chunk you are searching for. In this case NP stands for noun phrase.

Q10. Explain Named Entity Recognition

Ans:

Named entity recognition (NER) — sometimes referred to as entity chunking, extraction, or identification — is the task of identifying and categorizing key information (entities) in text. ... NER is a form of natural language processing (NLP), a subfield of artificial intelligence.
